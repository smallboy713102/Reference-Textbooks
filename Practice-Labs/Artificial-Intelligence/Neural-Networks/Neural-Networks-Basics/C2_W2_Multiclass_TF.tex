\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C2\_W2\_Multiclass\_TF}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{optional-lab---multi-class-classification}{%
\section{Optional Lab - Multi-class
Classification}\label{optional-lab---multi-class-classification}}

    \hypertarget{goals}{%
\subsection{1.1 Goals}\label{goals}}

In this lab, you will explore an example of multi-class classification
using neural networks.

    \hypertarget{tools}{%
\subsection{1.2 Tools}\label{tools}}

You will use some plotting routines. These are stored in
\texttt{lab\_utils\_multiclass\_TF.py} in this directory.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} widget
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}blobs}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k+kn}{import} \PY{n}{Sequential}
\PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k+kn}{import} \PY{n}{Dense}
\PY{n}{np}\PY{o}{.}\PY{n}{set\PYZus{}printoptions}\PY{p}{(}\PY{n}{precision}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
\PY{k+kn}{from} \PY{n+nn}{lab\PYZus{}utils\PYZus{}multiclass\PYZus{}TF} \PY{k+kn}{import} \PY{o}{*}
\PY{k+kn}{import} \PY{n+nn}{logging}
\PY{n}{logging}\PY{o}{.}\PY{n}{getLogger}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tensorflow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{setLevel}\PY{p}{(}\PY{n}{logging}\PY{o}{.}\PY{n}{ERROR}\PY{p}{)}
\PY{n}{tf}\PY{o}{.}\PY{n}{autograph}\PY{o}{.}\PY{n}{set\PYZus{}verbosity}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{multi-class-classification}{%
\section{2.0 Multi-class
Classification}\label{multi-class-classification}}

Neural Networks are often used to classify data. Examples are neural
networks: - take in photos and classify subjects in the photos as
\{dog,cat,horse,other\} - take in a sentence and classify the `parts of
speech' of its elements: \{noun, verb, adjective etc..\}

A network of this type will have multiple units in its final layer. Each
output is associated with a category. When an input example is applied
to the network, the output with the highest value is the category
predicted. If the output is applied to a softmax function, the output of
the softmax will provide probabilities of the input being in each
category.

In this lab you will see an example of building a multiclass network in
Tensorflow. We will then take a look at how the neural network makes its
predictions.

Let's start by creating a four-class data set.

    \hypertarget{prepare-and-visualize-our-data}{%
\subsection{2.1 Prepare and visualize our
data}\label{prepare-and-visualize-our-data}}

We will use Scikit-Learn \texttt{make\_blobs} function to make a
training data set with 4 categories as shown in the plot below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} make 4\PYZhy{}class dataset for classification}
\PY{n}{classes} \PY{o}{=} \PY{l+m+mi}{4}
\PY{n}{m} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{centers} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}
\PY{n}{std} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{make\PYZus{}blobs}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{n}{m}\PY{p}{,} \PY{n}{centers}\PY{o}{=}\PY{n}{centers}\PY{p}{,} \PY{n}{cluster\PYZus{}std}\PY{o}{=}\PY{n}{std}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt\PYZus{}mc}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{classes}\PY{p}{,} \PY{n}{centers}\PY{p}{,} \PY{n}{std}\PY{o}{=}\PY{n}{std}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{verbatim}
Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …
    \end{verbatim}

    
    Each dot represents a training example. The axis (x0,x1) are the inputs
and the color represents the class the example is associated with. Once
trained, the model will be presented with a new example, (x0,x1), and
will predict the class.

While generated, this data set is representative of many real-world
classification problems. There are several input features (x0,\ldots,xn)
and several output categories. The model is trained to use the input
features to predict the correct output category.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} show classes in data set}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{unique classes }\PY{l+s+si}{\PYZob{}}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} show how classes are represented}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{class representation }\PY{l+s+si}{\PYZob{}}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} show shapes of our dataset}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape of X\PYZus{}train: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, shape of y\PYZus{}train: }\PY{l+s+si}{\PYZob{}}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
unique classes [0 1 2 3]
class representation [3 3 3 0 3 3 3 3 2 0]
shape of X\_train: (100, 2), shape of y\_train: (100,)
    \end{Verbatim}

    \hypertarget{model}{%
\subsection{2.2 Model}\label{model}}

This lab will use a 2-layer network as shown. Unlike the binary
classification networks, this network has four outputs, one for each
class. Given an input example, the output with the highest value is the
predicted class of the input.

Below is an example of how to construct this network in Tensorflow.
Notice the output layer uses a \texttt{linear} rather than a
\texttt{softmax} activation. While it is possible to include the softmax
in the output layer, it is more numerically stable if linear outputs are
passed to the loss function during training. If the model is used to
predict probabilities, the softmax can be applied at that point.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tf}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{set\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{1234}\PY{p}{)}  \PY{c+c1}{\PYZsh{} applied to achieve consistent results}
\PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}
    \PY{p}{[}
        \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}   \PY{n}{name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{p}{]}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    The statements below compile and train the network. Setting
\texttt{from\_logits=True} as an argument to the loss function specifies
that the output activation was linear rather than a softmax.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
    \PY{n}{loss}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{SparseCategoricalCrossentropy}\PY{p}{(}\PY{n}{from\PYZus{}logits}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
    \PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,}
\PY{p}{)}

\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}
    \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{200}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/200
4/4 [==============================] - 0s 1ms/step - loss: 1.8158
Epoch 2/200
4/4 [==============================] - 0s 1ms/step - loss: 1.6976
Epoch 3/200
4/4 [==============================] - 0s 1ms/step - loss: 1.5989
Epoch 4/200
4/4 [==============================] - 0s 1ms/step - loss: 1.5179
Epoch 5/200
4/4 [==============================] - 0s 1ms/step - loss: 1.4369
Epoch 6/200
4/4 [==============================] - 0s 1ms/step - loss: 1.3756
Epoch 7/200
4/4 [==============================] - 0s 1ms/step - loss: 1.3154
Epoch 8/200
4/4 [==============================] - 0s 1ms/step - loss: 1.2621
Epoch 9/200
4/4 [==============================] - 0s 1ms/step - loss: 1.2188
Epoch 10/200
4/4 [==============================] - 0s 1ms/step - loss: 1.1791
Epoch 11/200
4/4 [==============================] - 0s 1ms/step - loss: 1.1446
Epoch 12/200
4/4 [==============================] - 0s 1ms/step - loss: 1.1129
Epoch 13/200
4/4 [==============================] - 0s 1ms/step - loss: 1.0827
Epoch 14/200
4/4 [==============================] - 0s 971us/step - loss: 1.0516
Epoch 15/200
4/4 [==============================] - 0s 991us/step - loss: 1.0225
Epoch 16/200
4/4 [==============================] - 0s 993us/step - loss: 0.9967
Epoch 17/200
4/4 [==============================] - 0s 1ms/step - loss: 0.9681
Epoch 18/200
4/4 [==============================] - 0s 979us/step - loss: 0.9392
Epoch 19/200
4/4 [==============================] - 0s 1ms/step - loss: 0.9092
Epoch 20/200
4/4 [==============================] - 0s 979us/step - loss: 0.8771
Epoch 21/200
4/4 [==============================] - 0s 981us/step - loss: 0.8461
Epoch 22/200
4/4 [==============================] - 0s 1ms/step - loss: 0.8099
Epoch 23/200
4/4 [==============================] - 0s 1ms/step - loss: 0.7771
Epoch 24/200
4/4 [==============================] - 0s 957us/step - loss: 0.7485
Epoch 25/200
4/4 [==============================] - 0s 974us/step - loss: 0.7215
Epoch 26/200
4/4 [==============================] - 0s 1ms/step - loss: 0.6967
Epoch 27/200
4/4 [==============================] - 0s 1ms/step - loss: 0.6742
Epoch 28/200
4/4 [==============================] - 0s 954us/step - loss: 0.6540
Epoch 29/200
4/4 [==============================] - 0s 966us/step - loss: 0.6352
Epoch 30/200
4/4 [==============================] - 0s 969us/step - loss: 0.6187
Epoch 31/200
4/4 [==============================] - 0s 1ms/step - loss: 0.6030
Epoch 32/200
4/4 [==============================] - 0s 985us/step - loss: 0.5884
Epoch 33/200
4/4 [==============================] - 0s 973us/step - loss: 0.5746
Epoch 34/200
4/4 [==============================] - 0s 999us/step - loss: 0.5621
Epoch 35/200
4/4 [==============================] - 0s 973us/step - loss: 0.5512
Epoch 36/200
4/4 [==============================] - 0s 982us/step - loss: 0.5414
Epoch 37/200
4/4 [==============================] - 0s 981us/step - loss: 0.5323
Epoch 38/200
4/4 [==============================] - 0s 1ms/step - loss: 0.5236
Epoch 39/200
4/4 [==============================] - 0s 977us/step - loss: 0.5150
Epoch 40/200
4/4 [==============================] - 0s 978us/step - loss: 0.5072
Epoch 41/200
4/4 [==============================] - 0s 1ms/step - loss: 0.5006
Epoch 42/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4944
Epoch 43/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4888
Epoch 44/200
4/4 [==============================] - 0s 998us/step - loss: 0.4830
Epoch 45/200
4/4 [==============================] - 0s 986us/step - loss: 0.4775
Epoch 46/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4725
Epoch 47/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4673
Epoch 48/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4624
Epoch 49/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4574
Epoch 50/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4530
Epoch 51/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4491
Epoch 52/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4451
Epoch 53/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4414
Epoch 54/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4374
Epoch 55/200
4/4 [==============================] - 0s 989us/step - loss: 0.4336
Epoch 56/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4295
Epoch 57/200
4/4 [==============================] - 0s 998us/step - loss: 0.4261
Epoch 58/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4225
Epoch 59/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4193
Epoch 60/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4161
Epoch 61/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4131
Epoch 62/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4098
Epoch 63/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4067
Epoch 64/200
4/4 [==============================] - 0s 1ms/step - loss: 0.4029
Epoch 65/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3994
Epoch 66/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3957
Epoch 67/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3920
Epoch 68/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3878
Epoch 69/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3837
Epoch 70/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3792
Epoch 71/200
4/4 [==============================] - 0s 991us/step - loss: 0.3755
Epoch 72/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3718
Epoch 73/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3683
Epoch 74/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3643
Epoch 75/200
4/4 [==============================] - 0s 997us/step - loss: 0.3600
Epoch 76/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3550
Epoch 77/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3491
Epoch 78/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3425
Epoch 79/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3367
Epoch 80/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3293
Epoch 81/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3228
Epoch 82/200
4/4 [==============================] - 0s 967us/step - loss: 0.3156
Epoch 83/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3080
Epoch 84/200
4/4 [==============================] - 0s 1ms/step - loss: 0.3006
Epoch 85/200
4/4 [==============================] - 0s 1ms/step - loss: 0.2933
Epoch 86/200
4/4 [==============================] - 0s 995us/step - loss: 0.2864
Epoch 87/200
4/4 [==============================] - 0s 1000us/step - loss: 0.2792
Epoch 88/200
4/4 [==============================] - 0s 971us/step - loss: 0.2720
Epoch 89/200
4/4 [==============================] - 0s 954us/step - loss: 0.2645
Epoch 90/200
4/4 [==============================] - 0s 994us/step - loss: 0.2570
Epoch 91/200
4/4 [==============================] - 0s 994us/step - loss: 0.2498
Epoch 92/200
4/4 [==============================] - 0s 1ms/step - loss: 0.2432
Epoch 93/200
4/4 [==============================] - 0s 1ms/step - loss: 0.2354
Epoch 94/200
4/4 [==============================] - 0s 995us/step - loss: 0.2274
Epoch 95/200
4/4 [==============================] - 0s 986us/step - loss: 0.2194
Epoch 96/200
4/4 [==============================] - 0s 985us/step - loss: 0.2127
Epoch 97/200
4/4 [==============================] - 0s 993us/step - loss: 0.2060
Epoch 98/200
4/4 [==============================] - 0s 981us/step - loss: 0.1995
Epoch 99/200
4/4 [==============================] - 0s 992us/step - loss: 0.1950
Epoch 100/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1894
Epoch 101/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1850
Epoch 102/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1804
Epoch 103/200
4/4 [==============================] - 0s 998us/step - loss: 0.1758
Epoch 104/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1709
Epoch 105/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1662
Epoch 106/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1616
Epoch 107/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1575
Epoch 108/200
4/4 [==============================] - 0s 994us/step - loss: 0.1527
Epoch 109/200
4/4 [==============================] - 0s 979us/step - loss: 0.1480
Epoch 110/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1439
Epoch 111/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1396
Epoch 112/200
4/4 [==============================] - 0s 979us/step - loss: 0.1357
Epoch 113/200
4/4 [==============================] - 0s 986us/step - loss: 0.1315
Epoch 114/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1277
Epoch 115/200
4/4 [==============================] - 0s 986us/step - loss: 0.1240
Epoch 116/200
4/4 [==============================] - 0s 959us/step - loss: 0.1207
Epoch 117/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1171
Epoch 118/200
4/4 [==============================] - 0s 991us/step - loss: 0.1139
Epoch 119/200
4/4 [==============================] - 0s 994us/step - loss: 0.1110
Epoch 120/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1084
Epoch 121/200
4/4 [==============================] - 0s 985us/step - loss: 0.1058
Epoch 122/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1029
Epoch 123/200
4/4 [==============================] - 0s 1ms/step - loss: 0.1001
Epoch 124/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0975
Epoch 125/200
4/4 [==============================] - 0s 1000us/step - loss: 0.0951
Epoch 126/200
4/4 [==============================] - 0s 963us/step - loss: 0.0925
Epoch 127/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0902
Epoch 128/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0882
Epoch 129/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0862
Epoch 130/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0840
Epoch 131/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0826
Epoch 132/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0807
Epoch 133/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0790
Epoch 134/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0772
Epoch 135/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0757
Epoch 136/200
4/4 [==============================] - 0s 983us/step - loss: 0.0741
Epoch 137/200
4/4 [==============================] - 0s 952us/step - loss: 0.0728
Epoch 138/200
4/4 [==============================] - 0s 1000us/step - loss: 0.0714
Epoch 139/200
4/4 [==============================] - 0s 981us/step - loss: 0.0700
Epoch 140/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0685
Epoch 141/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0670
Epoch 142/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0657
Epoch 143/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0645
Epoch 144/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0634
Epoch 145/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0622
Epoch 146/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0611
Epoch 147/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0601
Epoch 148/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0591
Epoch 149/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0582
Epoch 150/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0575
Epoch 151/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0570
Epoch 152/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0563
Epoch 153/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0553
Epoch 154/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0541
Epoch 155/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0530
Epoch 156/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0519
Epoch 157/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0510
Epoch 158/200
4/4 [==============================] - 0s 990us/step - loss: 0.0502
Epoch 159/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0496
Epoch 160/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0490
Epoch 161/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0481
Epoch 162/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0473
Epoch 163/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0466
Epoch 164/200
4/4 [==============================] - 0s 982us/step - loss: 0.0458
Epoch 165/200
4/4 [==============================] - 0s 982us/step - loss: 0.0452
Epoch 166/200
4/4 [==============================] - 0s 996us/step - loss: 0.0445
Epoch 167/200
4/4 [==============================] - 0s 974us/step - loss: 0.0440
Epoch 168/200
4/4 [==============================] - 0s 961us/step - loss: 0.0434
Epoch 169/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0429
Epoch 170/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0423
Epoch 171/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0418
Epoch 172/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0414
Epoch 173/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0413
Epoch 174/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0408
Epoch 175/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0401
Epoch 176/200
4/4 [==============================] - 0s 998us/step - loss: 0.0393
Epoch 177/200
4/4 [==============================] - 0s 966us/step - loss: 0.0388
Epoch 178/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0384
Epoch 179/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0384
Epoch 180/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0377
Epoch 181/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0369
Epoch 182/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0366
Epoch 183/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0363
Epoch 184/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0359
Epoch 185/200
4/4 [==============================] - 0s 994us/step - loss: 0.0353
Epoch 186/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0348
Epoch 187/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0345
Epoch 188/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0343
Epoch 189/200
4/4 [==============================] - 0s 988us/step - loss: 0.0339
Epoch 190/200
4/4 [==============================] - 0s 971us/step - loss: 0.0337
Epoch 191/200
4/4 [==============================] - 0s 991us/step - loss: 0.0333
Epoch 192/200
4/4 [==============================] - 0s 984us/step - loss: 0.0330
Epoch 193/200
4/4 [==============================] - 0s 984us/step - loss: 0.0325
Epoch 194/200
4/4 [==============================] - 0s 997us/step - loss: 0.0321
Epoch 195/200
4/4 [==============================] - 0s 980us/step - loss: 0.0317
Epoch 196/200
4/4 [==============================] - 0s 1ms/step - loss: 0.0314
Epoch 197/200
4/4 [==============================] - 0s 986us/step - loss: 0.0310
Epoch 198/200
4/4 [==============================] - 0s 989us/step - loss: 0.0306
Epoch 199/200
4/4 [==============================] - 0s 987us/step - loss: 0.0303
Epoch 200/200
4/4 [==============================] - 0s 993us/step - loss: 0.0300
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<keras.callbacks.History at 0x7f2cf073c690>
\end{Verbatim}
\end{tcolorbox}
        
    With the model trained, we can see how the model has classified the
training data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt\PYZus{}cat\PYZus{}mc}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{classes}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{verbatim}
Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …
    \end{verbatim}

    
    Above, the decision boundaries show how the model has partitioned the
input space. This very simple model has had no trouble classifying the
training data. How did it accomplish this? Let's look at the network in
more detail.

Below, we will pull the trained weights from the model and use that to
plot the function of each of the network units. Further down, there is a
more detailed explanation of the results. You don't need to know these
details to successfully use neural networks, but it may be helpful to
gain more intuition about how the layers combine to solve a
classification problem.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} gather the trained parameters from the first layer}
\PY{n}{l1} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}layer}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{W1}\PY{p}{,}\PY{n}{b1} \PY{o}{=} \PY{n}{l1}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot the function of the first layer}
\PY{n}{plt\PYZus{}layer\PYZus{}relu}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{W1}\PY{p}{,} \PY{n}{b1}\PY{p}{,} \PY{n}{classes}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{verbatim}
Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …
    \end{verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} gather the trained parameters from the output layer}
\PY{n}{l2} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}layer}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{W2}\PY{p}{,} \PY{n}{b2} \PY{o}{=} \PY{n}{l2}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{} create the \PYZsq{}new features\PYZsq{}, the training examples after L1 transformation}
\PY{n}{Xl2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{W1}\PY{p}{)} \PY{o}{+} \PY{n}{b1}\PY{p}{)}

\PY{n}{plt\PYZus{}output\PYZus{}layer\PYZus{}linear}\PY{p}{(}\PY{n}{Xl2}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{W2}\PY{p}{,} \PY{n}{b2}\PY{p}{,} \PY{n}{classes}\PY{p}{,}
                        \PY{n}{x0\PYZus{}rng} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.25}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n}{Xl2}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{x1\PYZus{}rng} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.25}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n}{Xl2}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{verbatim}
Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …
    \end{verbatim}

    
    \hypertarget{explanation}{%
\subsection{Explanation}\label{explanation}}

\hypertarget{layer-1}{%
\paragraph{\texorpdfstring{Layer 1 }{Layer 1 }}\label{layer-1}}

These plots show the function of Units 0 and 1 in the first layer of the
network. The inputs are (\(x_0,x_1\)) on the axis. The output of the
unit is represented by the color of the background. This is indicated by
the color bar on the right of each graph. Notice that since these units
are using a ReLu, the outputs do not necessarily fall between 0 and 1
and in this case are greater than 20 at their peaks. The contour lines
in this graph show the transition point between the output,
\(a^{[1]}_j\) being zero and non-zero. Recall the graph for a ReLu : The
contour line in the graph is the inflection point in the ReLu.

Unit 0 has separated classes 0 and 1 from classes 2 and 3. Points to the
left of the line (classes 0 and 1) will output zero, while points to the
right will output a value greater than zero.\\
Unit 1 has separated classes 0 and 2 from classes 1 and 3. Points above
the line (classes 0 and 2 ) will output a zero, while points below will
output a value greater than zero. Let's see how this works out in the
next layer!

    \hypertarget{layer-2-the-output-layer}{%
\paragraph{\texorpdfstring{Layer 2, the output layer
}{Layer 2, the output layer }}\label{layer-2-the-output-layer}}

The dots in these graphs are the training examples translated by the
first layer. One way to think of this is the first layer has created a
new set of features for evaluation by the 2nd layer. The axes in these
plots are the outputs of the previous layer \(a^{[1]}_0\) and
\(a^{[1]}_1\). As predicted above, classes 0 and 1 (blue and green) have
\(a^{[1]}_0 = 0\) while classes 0 and 2 (blue and orange) have
\(a^{[1]}_1 = 0\).\\
Once again, the intensity of the background color indicates the highest
values.\\
Unit 0 will produce its maximum value for values near (0,0), where class
0 (blue) has been mapped.\\
Unit 1 produces its highest values in the upper left corner selecting
class 1 (green).\\
Unit 2 targets the lower right corner where class 2 (orange) resides.\\
Unit 3 produces its highest values in the upper right selecting our
final class (purple).

One other aspect that is not obvious from the graphs is that the values
have been coordinated between the units. It is not sufficient for a unit
to produce a maximum value for the class it is selecting for, it must
also be the highest value of all the units for points in that class.
This is done by the implied softmax function that is part of the loss
function (\texttt{SparseCategoricalCrossEntropy}). Unlike other
activation functions, the softmax works across all the outputs.

You can successfully use neural networks without knowing the details of
what each unit is up to. Hopefully, this example has provided some
intuition about what is happening under the hood.

    \hypertarget{congratulations}{%
\subsection{Congratulations!}\label{congratulations}}

You have learned to build and operate a neural network for multiclass
classification.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
