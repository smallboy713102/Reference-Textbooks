\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C1\_W3\_Logistic\_Regression}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{logistic-regression}{%
\section{Logistic Regression}\label{logistic-regression}}

In this exercise, you will implement logistic regression and apply it to
two different datasets.

\hypertarget{outline}{%
\section{Outline}\label{outline}}

\begin{itemize}
\tightlist
\item
  Section \ref{1}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{21}
  \item
    Section \ref{22}
  \item
    Section \ref{23}
  \item
    Section \ref{24}
  \item
    Section \ref{25}
  \item
    Section \ref{26}
  \item
    Section \ref{27}
  \item
    Section \ref{28}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{31}
  \item
    Section \ref{32}
  \item
    Section \ref{33}
  \item
    Section \ref{34}
  \item
    Section \ref{35}
  \item
    Section \ref{36}
  \item
    Section \ref{37}
  \item
    Section \ref{38}
  \end{itemize}
\end{itemize}

    \#\# 1 - Packages

First, let's run the cell below to import all the packages that you will
need during this assignment. - \href{www.numpy.org}{numpy} is the
fundamental package for scientific computing with Python. -
\href{http://matplotlib.org}{matplotlib} is a famous library to plot
graphs in Python. - \texttt{utils.py} contains helper functions for this
assignment. You do not need to modify code in this file.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{utils} \PY{k+kn}{import} \PY{o}{*}
\PY{k+kn}{import} \PY{n+nn}{copy}
\PY{k+kn}{import} \PY{n+nn}{math}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}
\end{tcolorbox}

    \#\# 2 - Logistic Regression

In this part of the exercise, you will build a logistic regression model
to predict whether a student gets admitted into a university.

\#\#\# 2.1 Problem Statement

Suppose that you are the administrator of a university department and
you want to determine each applicant's chance of admission based on
their results on two exams. * You have historical data from previous
applicants that you can use as a training set for logistic regression. *
For each training example, you have the applicant's scores on two exams
and the admissions decision. * Your task is to build a classification
model that estimates an applicant's probability of admission based on
the scores from those two exams.

\#\#\# 2.2 Loading and visualizing the data

You will start by loading the dataset for this task. - The
\texttt{load\_dataset()} function shown below loads the data into
variables \texttt{X\_train} and \texttt{y\_train} - \texttt{X\_train}
contains exam scores on two exams for a student - \texttt{y\_train} is
the admission decision - \texttt{y\_train\ =\ 1} if the student was
admitted - \texttt{y\_train\ =\ 0} if the student was not admitted -
Both \texttt{X\_train} and \texttt{y\_train} are numpy arrays.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} load dataset}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{load\PYZus{}data}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ex2data1.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{view-the-variables}{%
\paragraph{View the variables}\label{view-the-variables}}

Let's get more familiar with your dataset.\\
- A good place to start is to just print out each variable and see what
it contains.

The code below prints the first five values of \texttt{X\_train} and the
type of the variable.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First five elements in X\PYZus{}train are:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Type of X\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
First five elements in X\_train are:
 [[34.62365962 78.02469282]
 [30.28671077 43.89499752]
 [35.84740877 72.90219803]
 [60.18259939 86.3085521 ]
 [79.03273605 75.34437644]]
Type of X\_train: <class 'numpy.ndarray'>
    \end{Verbatim}

    Now print the first five values of \texttt{y\_train}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First five elements in y\PYZus{}train are:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Type of y\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
First five elements in y\_train are:
 [0. 0. 0. 1. 1.]
Type of y\_train: <class 'numpy.ndarray'>
    \end{Verbatim}

    \hypertarget{check-the-dimensions-of-your-variables}{%
\paragraph{Check the dimensions of your
variables}\label{check-the-dimensions-of-your-variables}}

Another useful way to get familiar with your data is to view its
dimensions. Let's print the shape of \texttt{X\_train} and
\texttt{y\_train} and see how many training examples we have in our
dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The shape of X\PYZus{}train is: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The shape of y\PYZus{}train is: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{We have m = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ training examples}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The shape of X\_train is: (100, 2)
The shape of y\_train is: (100,)
We have m = 100 training examples
    \end{Verbatim}

    \hypertarget{visualize-your-data}{%
\paragraph{Visualize your data}\label{visualize-your-data}}

Before starting to implement any learning algorithm, it is always good
to visualize the data if possible. - The code below displays the data on
a 2D plot (as shown below), where the axes are the two exam scores, and
the positive and negative examples are shown with different markers. -
We use a helper function in the \texttt{utils.py} file to generate this
plot.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot examples}
\PY{n}{plot\PYZus{}data}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{pos\PYZus{}label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Admitted}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{neg\PYZus{}label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not admitted}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Set the y\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exam 2 score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{c+c1}{\PYZsh{} Set the x\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exam 1 score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{upper right}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Your goal is to build a logistic regression model to fit this data. -
With this model, you can then predict if a new student will be admitted
based on their scores on the two exams.

    \#\#\# 2.3 Sigmoid function

Recall that for logistic regression, the model is represented as

\[ f_{\mathbf{w},b}(x) = g(\mathbf{w}\cdot \mathbf{x} + b)\] where
function \(g\) is the sigmoid function. The sigmoid function is defined
as:

\[g(z) = \frac{1}{1+e^{-z}}\]

Let's implement the sigmoid function first, so it can be used by the
rest of this assignment.

\#\#\# Exercise 1 Please complete the \texttt{sigmoid} function to
calculate

\[g(z) = \frac{1}{1+e^{-z}}\]

Note that - \texttt{z} is not always a single number, but can also be an
array of numbers. - If the input is an array of numbers, we'd like to
apply the sigmoid function to each value in the input array.

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: sigmoid}

\PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Compute the sigmoid of z}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        z (ndarray): A scalar, numpy array of any size.}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        g (ndarray): sigmoid(z), with the same shape as z}
\PY{l+s+sd}{         }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
          
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }
    \PY{n}{g} \PY{o}{=} \PY{l+m+mf}{1.0}\PY{o}{/}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{z}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END SOLUTION \PYZsh{}\PYZsh{}\PYZsh{}  }
    
    \PY{k}{return} \PY{n}{g}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\texttt{numpy} has a function called
\href{https://numpy.org/doc/stable/reference/generated/numpy.exp.html}{\texttt{np.exp()}},
which offers a convinient way to calculate the exponential ( \(e^{z}\))
of all elements in the input array (\texttt{z}).

Click for more hints

\begin{itemize}
\item
  You can translate \(e^{-z}\) into code as \texttt{np.exp(-z)}

  \begin{itemize}
  \item
    You can translate \(1/e^{-z}\) into code as \texttt{1/np.exp(-z)}

    If you're still stuck, you can check the hints presented below to
    figure out how to calculate \texttt{g}

    Hint to calculate g g = 1 / (1 + np.exp(-z))
  \end{itemize}
\end{itemize}

    When you are finished, try testing a few values by calling
\texttt{sigmoid(x)} in the cell below. - For large positive values of x,
the sigmoid should be close to 1, while for large negative values, the
sigmoid should be close to 0. - Evaluating \texttt{sigmoid(0)} should
give you exactly 0.5.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid(0) = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
sigmoid(0) = 0.5
    \end{Verbatim}

    \textbf{Expected Output}:

sigmoid(0)

0.5

\begin{itemize}
\tightlist
\item
  As mentioned before, your code should also work with vectors and
  matrices. For a matrix, your function should perform the sigmoid
  function on every element.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid([ \PYZhy{}1, 0, 1, 2]) = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} UNIT TESTS}
\PY{k+kn}{from} \PY{n+nn}{public\PYZus{}tests} \PY{k+kn}{import} \PY{o}{*}
\PY{n}{sigmoid\PYZus{}test}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
sigmoid([ -1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected Output}:

sigmoid({[}-1, 0, 1, 2{]})

{[}0.26894142 0.5 0.73105858 0.88079708{]}

    \#\#\# 2.4 Cost function for logistic regression

In this section, you will implement the cost function for logistic
regression.

\#\#\# Exercise 2

Please complete the \texttt{compute\_cost} function using the equations
below.

Recall that for logistic regression, the cost function is of the form

\[ J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}\]

where * m is the number of training examples in the dataset

\begin{itemize}
\item
  \(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\) is the cost for
  a single data point, which is -

  \[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}\]
\item
  \(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\) is the model's prediction,
  while \(y^{(i)}\), which is the actual label
\item
  \(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x^{(i)}} + b)\)
  where function \(g\) is the sigmoid function.

  \begin{itemize}
  \tightlist
  \item
    It might be helpful to first calculate an intermediate variable
    \(z_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b\)
    where \(n\) is the number of features, before calculating
    \(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)
  \end{itemize}
\end{itemize}

Note: * As you are doing this, remember that the variables
\texttt{X\_train} and \texttt{y\_train} are not scalar values but
matrices of shape (\(m, n\)) and (\(𝑚\),1) respectively, where \(𝑛\) is
the number of features and \(𝑚\) is the number of training examples. *
You can use the sigmoid function that you implemented above for this
part.

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{model\PYZus{}equation}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n}{b} 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{calc\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}predict}\PY{p}{,} \PY{n}{y\PYZus{}actual}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}actual} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{y\PYZus{}predict}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}actual}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}predict}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}cost}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the cost over all examples}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      X : (ndarray Shape (m,n)) data, m examples by n features}
\PY{l+s+sd}{      y : (array\PYZus{}like Shape (m,)) target value }
\PY{l+s+sd}{      w : (array\PYZus{}like Shape (n,)) Values of parameters of the model      }
\PY{l+s+sd}{      b : scalar Values of bias parameter of the model}
\PY{l+s+sd}{      lambda\PYZus{}: unused placeholder}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{      total\PYZus{}cost: (scalar)         cost }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{n}{m}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{n}{total\PYZus{}cost}  \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{calc\PYZus{}error}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{model\PYZus{}equation}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{m}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }

    \PY{k}{return} \PY{n}{total\PYZus{}cost}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  You can represent a summation operator eg:
  \(h = \sum\limits_{i = 0}^{m-1} 2i\) in code as follows:
  \texttt{python\ \ \ \ \ \ \ \ \ \ h\ =\ 0\ \ \ \ \ \ \ \ \ for\ i\ in\ range(m):\ \ \ \ \ \ \ \ \ \ \ \ \ h\ =\ h\ +\ 2*i}

  \begin{itemize}
  \item
    In this case, you can iterate over all the examples in \texttt{X}
    using a for loop and add the \texttt{loss} from each iteration to a
    variable (\texttt{loss\_sum}) initialized outside the loop.
  \item
    Then, you can return the \texttt{total\_cost} as \texttt{loss\_sum}
    divided by \texttt{m}.
  \end{itemize}

  Click for more hints

  \begin{itemize}
  \tightlist
  \item
    Here's how you can structure the overall implementation for this
    function
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{def}\NormalTok{ compute\_cost(X, y, w, b, lambda\_}\OperatorTok{=} \DecValTok{1}\NormalTok{):}
\NormalTok{     m, n }\OperatorTok{=}\NormalTok{ X.shape}

     \CommentTok{\#\#\# START CODE HERE }\AlertTok{\#\#\#}
\NormalTok{     loss\_sum }\OperatorTok{=} \DecValTok{0} 

     \CommentTok{\# Loop over each training example}
     \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m): }

         \CommentTok{\# First calculate z\_wb = w[0]*X[i][0]+...+w[n{-}1]*X[i][n{-}1]+b}
\NormalTok{         z\_wb }\OperatorTok{=} \DecValTok{0} 
         \CommentTok{\# Loop over each feature}
         \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n): }
             \CommentTok{\# Add the corresponding term to z\_wb}
\NormalTok{             z\_wb\_ij }\OperatorTok{=} \CommentTok{\# Your code here to calculate w[j] * X[i][j]}
\NormalTok{             z\_wb }\OperatorTok{+=}\NormalTok{ z\_wb\_ij }\CommentTok{\# equivalent to z\_wb = z\_wb + z\_wb\_ij}
         \CommentTok{\# Add the bias term to z\_wb}
\NormalTok{         z\_wb }\OperatorTok{+=}\NormalTok{ b }\CommentTok{\# equivalent to z\_wb = z\_wb + b}

\NormalTok{         f\_wb }\OperatorTok{=} \CommentTok{\# Your code here to calculate prediction f\_wb for a training example}
\NormalTok{         loss }\OperatorTok{=}  \CommentTok{\# Your code here to calculate loss for a training example}

\NormalTok{         loss\_sum }\OperatorTok{+=}\NormalTok{ loss }\CommentTok{\# equivalent to loss\_sum = loss\_sum + loss}

\NormalTok{     total\_cost }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{/}\NormalTok{ m) }\OperatorTok{*}\NormalTok{ loss\_sum  }
     \CommentTok{\#\#\# }\RegionMarkerTok{END}\CommentTok{ CODE HERE }\AlertTok{\#\#\#}\CommentTok{ }

     \ControlFlowTok{return}\NormalTok{ total\_cost}
\end{Highlighting}
\end{Shaded}

  If you're still stuck, you can check the hints presented below to
  figure out how to calculate \texttt{z\_wb\_ij}, \texttt{f\_wb} and
  \texttt{cost}.

  Hint to calculate z\_wb\_ij     z\_wb\_ij = w{[}j{]}*X{[}i{]}{[}j{]}

  Hint to calculate f\_wb    
  \(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)
  where \(g\) is the sigmoid function. You can simply call the
  \texttt{sigmoid} function implemented above.

      More hints to calculate f     You can compute f\_wb as f\_wb =
  sigmoid(z\_wb)

  Hint to calculate loss     You can use the np.log function to
  calculate the log

      More hints to calculate loss     You can compute loss as loss =
  -y{[}i{]} * np.log(f\_wb) - (1 - y{[}i{]}) * np.log(1 - f\_wb)
\end{itemize}

    Run the cells below to check your implementation of the
\texttt{compute\_cost} function with two different initializations of
the parameters \(w\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{m}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}

\PY{c+c1}{\PYZsh{} Compute and display cost with w initialized to zeroes}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n}\PY{p}{)}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.}
\PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cost at initial w (zeros): }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{cost}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost at initial w (zeros): 0.693
    \end{Verbatim}

    \textbf{Expected Output}:

Cost at initial w (zeros)

0.693

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute and display cost with non\PYZhy{}zero w}
\PY{n}{test\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{)}
\PY{n}{test\PYZus{}b} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{24.}
\PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{test\PYZus{}w}\PY{p}{,} \PY{n}{test\PYZus{}b}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cost at test w,b: }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{cost}\PY{p}{)}\PY{p}{)}


\PY{c+c1}{\PYZsh{} UNIT TESTS}
\PY{n}{compute\PYZus{}cost\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cost at test w,b: 0.218
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected Output}:

Cost at test w,b

0.218

    \#\#\# 2.5 Gradient for logistic regression

In this section, you will implement the gradient for logistic
regression.

Recall that the gradient descent algorithm is:

\[\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & b := b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \newline       \; & w_j := w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; & \text{for j := 0..n-1}\newline & \rbrace\end{align*}\]

where, parameters \(b\), \(w_j\) are all updated simultaniously

    \#\#\# Exercise 3

Please complete the \texttt{compute\_gradient} function to compute
\(\frac{\partial J(\mathbf{w},b)}{\partial w}\),
\(\frac{\partial J(\mathbf{w},b)}{\partial b}\) from equations (2) and
(3) below.

\[
\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}
\] \[
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}
\] * m is the number of training examples in the dataset

\begin{itemize}
\item
  \(f_{\mathbf{w},b}(x^{(i)})\) is the model's prediction, while
  \(y^{(i)}\) is the actual label
\item
  \textbf{Note}: While this gradient looks identical to the linear
  regression gradient, the formula is actually different because linear
  and logistic regression have different definitions of
  \(f_{\mathbf{w},b}(x)\).
\end{itemize}

As before, you can use the sigmoid function that you implemented above
and if you get stuck, you can check out the hints presented after the
cell below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}gradient}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the gradient for logistic regression }
\PY{l+s+sd}{ }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      X : (ndarray Shape (m,n)) variable such as house size }
\PY{l+s+sd}{      y : (array\PYZus{}like Shape (m,1)) actual value }
\PY{l+s+sd}{      w : (array\PYZus{}like Shape (n,1)) values of parameters of the model      }
\PY{l+s+sd}{      b : (scalar)                 value of parameter of the model }
\PY{l+s+sd}{      lambda\PYZus{}: unused placeholder.}
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{      dj\PYZus{}dw: (array\PYZus{}like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. }
\PY{l+s+sd}{      dj\PYZus{}db: (scalar)                The gradient of the cost w.r.t. the parameter b. }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{m}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}
    \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
    \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{l+m+mf}{0.}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
        \PY{n}{f\PYZus{}wb\PYZus{}i} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)}          \PY{c+c1}{\PYZsh{}(n,)(n,)=scalar}
        \PY{n}{err\PYZus{}i}  \PY{o}{=} \PY{n}{f\PYZus{}wb\PYZus{}i}  \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}                       \PY{c+c1}{\PYZsh{}scalar}
        \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
            \PY{n}{dj\PYZus{}dw}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{dj\PYZus{}dw}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{+} \PY{n}{err\PYZus{}i} \PY{o}{*} \PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{]}      \PY{c+c1}{\PYZsh{}scalar}
        \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{n}{dj\PYZus{}db} \PY{o}{+} \PY{n}{err\PYZus{}i}
    \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{dj\PYZus{}dw}\PY{o}{/}\PY{n}{m}                                   \PY{c+c1}{\PYZsh{}(n,)}
    \PY{n}{dj\PYZus{}db} \PY{o}{=} \PY{n}{dj\PYZus{}db}\PY{o}{/}\PY{n}{m}                                   \PY{c+c1}{\PYZsh{}scalar}
        
    \PY{k}{return} \PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw} 
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        
    \PY{k}{return} \PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  Here's how you can structure the overall implementation for this
  function ```python def compute\_gradient(X, y, w, b, lambda\_=None):
  m, n = X.shape dj\_dw = np.zeros(w.shape) dj\_db = 0.

\begin{verbatim}
      ### START CODE HERE ### 
      for i in range(m):
          # Calculate f_wb (exactly as you did in the compute_cost function above)
          f_wb = 

          # Calculate the  gradient for b from this example
          dj_db_i = # Your code here to calculate the error

          # add that to dj_db
          dj_db += dj_db_i

          # get dj_dw for each attribute
          for j in range(n):
              # You code here to calculate the gradient from the i-th example for j-th attribute
              dj_dw_ij =  
              dj_dw[j] += dj_dw_ij

      # divide dj_db and dj_dw by total number of examples
      dj_dw = dj_dw / m
      dj_db = dj_db / m
      ### END CODE HERE ###

      return dj_db, dj_dw
\end{verbatim}

  ```

  If you're still stuck, you can check the hints presented below to
  figure out how to calculate \texttt{f\_wb}, \texttt{dj\_db\_i} and
  \texttt{dj\_dw\_ij}

  Hint to calculate f\_wb     Recall that you calculated f\_wb in
  compute\_cost above --- for detailed hints on how to calculate each
  intermediate term, check out the hints section below that exercise

      More hints to calculate f\_wb     You can calculate f\_wb as

  for i in range(m):\\
  \# Calculate f\_wb (exactly how you did it in the compute\_cost
  function above) z\_wb = 0 \# Loop over each feature for j in range(n):
  \# Add the corresponding term to z\_wb z\_wb\_ij = X{[}i, j{]} *
  w{[}j{]} z\_wb += z\_wb\_ij

\begin{verbatim}
# Add bias term 
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
\end{verbatim}

  Hint to calculate dj\_db\_i     You can calculate dj\_db\_i as
  dj\_db\_i = f\_wb - y{[}i{]}

  Hint to calculate dj\_dw\_ij     You can calculate dj\_dw\_ij as
  dj\_dw\_ij = (f\_wb - y{[}i{]})* X{[}i{]}{[}j{]}
\end{itemize}

    Run the cells below to check your implementation of the
\texttt{compute\_gradient} function with two different initializations
of the parameters \(w\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute and display gradient with w initialized to zeroes}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n}\PY{p}{)}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.}

\PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dj\PYZus{}db at initial w (zeros):}\PY{l+s+si}{\PYZob{}}\PY{n}{dj\PYZus{}db}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dj\PYZus{}dw at initial w (zeros):}\PY{l+s+si}{\PYZob{}}\PY{n}{dj\PYZus{}dw}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dj\_db at initial w (zeros):-0.1
dj\_dw at initial w (zeros):[-12.00921658929115, -11.262842205513591]
    \end{Verbatim}

    \textbf{Expected Output}:

dj\_db at initial w (zeros)

-0.1

ddj\_dw at initial w (zeros):

{[}-12.00921658929115, -11.262842205513591{]}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute and display cost and gradient with non\PYZhy{}zero w}
\PY{n}{test\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{)}
\PY{n}{test\PYZus{}b} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{24}
\PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw}  \PY{o}{=} \PY{n}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{test\PYZus{}w}\PY{p}{,} \PY{n}{test\PYZus{}b}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dj\PYZus{}db at test\PYZus{}w:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dj\PYZus{}db}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dj\PYZus{}dw at test\PYZus{}w:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dj\PYZus{}dw}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} UNIT TESTS    }
\PY{n}{compute\PYZus{}gradient\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}gradient}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dj\_db at test\_w: -0.5999999999991071
dj\_dw at test\_w: [-44.831353617873795, -44.37384124953978]
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected Output}:

dj\_db at initial w (zeros)

-0.5999999999991071

ddj\_dw at initial w (zeros):

{[}-44.8313536178737957, -44.37384124953978{]}

    \#\#\# 2.6 Learning parameters using gradient descent

Similar to the previous assignment, you will now find the optimal
parameters of a logistic regression model by using gradient descent. -
You don't need to implement anything for this part. Simply run the cells
below.

\begin{itemize}
\item
  A good way to verify that gradient descent is working correctly is to
  look at the value of \(J(\mathbf{w},b)\) and check that it is
  decreasing with each step.
\item
  Assuming you have implemented the gradient and computed the cost
  correctly, your value of \(J(\mathbf{w},b)\) should never increase,
  and should converge to a steady value by the end of the algorithm.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}in}\PY{p}{,} \PY{n}{b\PYZus{}in}\PY{p}{,} \PY{n}{cost\PYZus{}function}\PY{p}{,} \PY{n}{gradient\PYZus{}function}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{num\PYZus{}iters}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Performs batch gradient descent to learn theta. Updates theta by taking }
\PY{l+s+sd}{    num\PYZus{}iters gradient steps with learning rate alpha}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      X :    (array\PYZus{}like Shape (m, n)}
\PY{l+s+sd}{      y :    (array\PYZus{}like Shape (m,))}
\PY{l+s+sd}{      w\PYZus{}in : (array\PYZus{}like Shape (n,))  Initial values of parameters of the model}
\PY{l+s+sd}{      b\PYZus{}in : (scalar)                 Initial value of parameter of the model}
\PY{l+s+sd}{      cost\PYZus{}function:                  function to compute cost}
\PY{l+s+sd}{      alpha : (float)                 Learning rate}
\PY{l+s+sd}{      num\PYZus{}iters : (int)               number of iterations to run gradient descent}
\PY{l+s+sd}{      lambda\PYZus{} (scalar, float)         regularization constant}
\PY{l+s+sd}{      }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{      w : (array\PYZus{}like Shape (n,)) Updated values of parameters of the model after}
\PY{l+s+sd}{          running gradient descent}
\PY{l+s+sd}{      b : (scalar)                Updated value of parameter of the model after}
\PY{l+s+sd}{          running gradient descent}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} number of training examples}
    \PY{n}{m} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} An array to store cost J and w\PYZsq{}s at each iteration primarily for graphing later}
    \PY{n}{J\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{w\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}iters}\PY{p}{)}\PY{p}{:}

        \PY{c+c1}{\PYZsh{} Calculate the gradient and update the parameters}
        \PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{gradient\PYZus{}function}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}in}\PY{p}{,} \PY{n}{b\PYZus{}in}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}   

        \PY{c+c1}{\PYZsh{} Update Parameters using w, b, alpha and gradient}
        \PY{n}{w\PYZus{}in} \PY{o}{=} \PY{n}{w\PYZus{}in} \PY{o}{\PYZhy{}} \PY{n}{alpha} \PY{o}{*} \PY{n}{dj\PYZus{}dw}               
        \PY{n}{b\PYZus{}in} \PY{o}{=} \PY{n}{b\PYZus{}in} \PY{o}{\PYZhy{}} \PY{n}{alpha} \PY{o}{*} \PY{n}{dj\PYZus{}db}              
       
        \PY{c+c1}{\PYZsh{} Save cost J at each iteration}
        \PY{k}{if} \PY{n}{i}\PY{o}{\PYZlt{}}\PY{l+m+mi}{100000}\PY{p}{:}      \PY{c+c1}{\PYZsh{} prevent resource exhaustion }
            \PY{n}{cost} \PY{o}{=}  \PY{n}{cost\PYZus{}function}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}in}\PY{p}{,} \PY{n}{b\PYZus{}in}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}
            \PY{n}{J\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cost}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Print cost every at intervals 10 times or as many iterations if \PYZlt{} 10}
        \PY{k}{if} \PY{n}{i}\PY{o}{\PYZpc{}} \PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{num\PYZus{}iters}\PY{o}{/}\PY{l+m+mi}{10}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{i} \PY{o}{==} \PY{p}{(}\PY{n}{num\PYZus{}iters}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{n}{w\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{w\PYZus{}in}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{:}\PY{l+s+s2}{4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: Cost }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{float}\PY{p}{(}\PY{n}{J\PYZus{}history}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{8.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{   }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
    \PY{k}{return} \PY{n}{w\PYZus{}in}\PY{p}{,} \PY{n}{b\PYZus{}in}\PY{p}{,} \PY{n}{J\PYZus{}history}\PY{p}{,} \PY{n}{w\PYZus{}history} \PY{c+c1}{\PYZsh{}return w and J,w history for graphing}
\end{Verbatim}
\end{tcolorbox}

    Now let's run the gradient descent algorithm above to learn the
parameters for our dataset.

\textbf{Note}

The code block below takes a couple of minutes to run, especially with a
non-vectorized version. You can reduce the \texttt{iterations} to test
your implementation and iterate faster. If you have time, try running
100,000 iterations for better results.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{intial\PYZus{}w} \PY{o}{=} \PY{l+m+mf}{0.01} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.5}\PY{p}{)}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{8}


\PY{c+c1}{\PYZsh{} Some gradient descent settings}
\PY{n}{iterations} \PY{o}{=} \PY{l+m+mi}{10000}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n}{w}\PY{p}{,}\PY{n}{b}\PY{p}{,} \PY{n}{J\PYZus{}history}\PY{p}{,}\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{X\PYZus{}train} \PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{,} 
                                   \PY{n}{compute\PYZus{}cost}\PY{p}{,} \PY{n}{compute\PYZus{}gradient}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{iterations}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
    \end{Verbatim}

    Expected Output: Cost 0.30, (Click to see details):

\begin{verbatim}
# With the following settings
np.random.seed(1)
intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)
initial_b = -8
iterations = 10000
alpha = 0.001
#
\end{verbatim}

\begin{verbatim}
Iteration    0: Cost     1.01   
Iteration 1000: Cost     0.31   
Iteration 2000: Cost     0.30   
Iteration 3000: Cost     0.30   
Iteration 4000: Cost     0.30   
Iteration 5000: Cost     0.30   
Iteration 6000: Cost     0.30   
Iteration 7000: Cost     0.30   
Iteration 8000: Cost     0.30   
Iteration 9000: Cost     0.30   
Iteration 9999: Cost     0.30   
\end{verbatim}

    \#\#\# 2.7 Plotting the decision boundary

We will now use the final parameters from gradient descent to plot the
linear fit. If you implemented the previous parts correctly, you should
see the following plot:\\

We will use a helper function in the \texttt{utils.py} file to create
this plot.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \#\#\# 2.8 Evaluating logistic regression

We can evaluate the quality of the parameters we have found by seeing
how well the learned model predicts on our training set.

You will implement the \texttt{predict} function below to do this.

    \#\#\# Exercise 4

Please complete the \texttt{predict} function to produce \texttt{1} or
\texttt{0} predictions given a dataset and a learned parameter vector
\(w\) and \(b\). - First you need to compute the prediction from the
model \(f(x^{(i)}) = g(w \cdot x^{(i)})\) for every example - You've
implemented this before in the parts above - We interpret the output of
the model (\(f(x^{(i)})\)) as the probability that \(y^{(i)}=1\) given
\(x^{(i)}\) and parameterized by \(w\). - Therefore, to get a final
prediction (\(y^{(i)}=0\) or \(y^{(i)}=1\)) from the logistic regression
model, you can use the following heuristic -

if \(f(x^{(i)}) >= 0.5\), predict \(y^{(i)}=1\)

if \(f(x^{(i)}) < 0.5\), predict \(y^{(i)}=0\)

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: predict}

\PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Predict whether the label is 0 or 1 using learned logistic}
\PY{l+s+sd}{    regression parameters w}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{    X : (ndarray Shape (m, n))}
\PY{l+s+sd}{    w : (array\PYZus{}like Shape (n,))      Parameters of the model}
\PY{l+s+sd}{    b : (scalar, float)              Parameter of the model}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    p: (ndarray (m,1))}
\PY{l+s+sd}{        The predictions for X using a threshold at 0.5}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} number of training examples}
    \PY{n}{m}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}   
    \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{m}\PY{p}{)}
   
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }
    \PY{c+c1}{\PYZsh{} Loop over each example}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}   
        \PY{n}{z\PYZus{}wb} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{w}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Apply the threshold}
        \PY{n}{p}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0} \PY{k}{if} \PY{n}{z\PYZus{}wb} \PY{o}{\PYZlt{}} \PY{l+m+mf}{0.5} \PY{k}{else} \PY{l+m+mi}{1}
        
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }
    \PY{k}{return} \PY{n}{p}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  Here's how you can structure the overall implementation for this
  function ```python def predict(X, w, b): \# number of training
  examples m, n = X.shape\\
  p = np.zeros(m)

\begin{verbatim}
      ### START CODE HERE ### 
      # Loop over each example
      for i in range(m):   

          # Calculate f_wb (exactly how you did it in the compute_cost function above) 
          # using a couple of lines of code
          f_wb = 

          # Calculate the prediction for that training example 
          p[i] = # Your code here to calculate the prediction based on f_wb

      ### END CODE HERE ### 
      return p
\end{verbatim}

  ```

  If you're still stuck, you can check the hints presented below to
  figure out how to calculate \texttt{f\_wb} and \texttt{p{[}i{]}}

  Hint to calculate f\_wb     Recall that you calculated f\_wb in
  compute\_cost above --- for detailed hints on how to calculate each
  intermediate term, check out the hints section below that exercise

      More hints to calculate f\_wb     You can calculate f\_wb as

  for i in range(m):\\
  \# Calculate f\_wb (exactly how you did it in the compute\_cost
  function above) z\_wb = 0 \# Loop over each feature for j in range(n):
  \# Add the corresponding term to z\_wb z\_wb\_ij = X{[}i, j{]} *
  w{[}j{]} z\_wb += z\_wb\_ij

\begin{verbatim}
# Add bias term 
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
\end{verbatim}

  Hint to calculate p{[}i{]}     As an example, if you'd like to say x =
  1 if y is less than 3 and 0 otherwise, you can express it in code as x
  = y \textless{} 3 . Now do the same for p{[}i{]} = 1 if f\_wb
  \textgreater= 0.5 and 0 otherwise.

      More hints to calculate p{[}i{]}     You can compute p{[}i{]} as
  p{[}i{]} = f\_wb \textgreater= 0.5
\end{itemize}

    Once you have completed the function \texttt{predict}, let's run the
code below to report the training accuracy of your classifier by
computing the percentage of examples it got correct.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test your predict code}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{tmp\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{tmp\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.3}    
\PY{n}{tmp\PYZus{}X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.5}

\PY{n}{tmp\PYZus{}p} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{tmp\PYZus{}X}\PY{p}{,} \PY{n}{tmp\PYZus{}w}\PY{p}{,} \PY{n}{tmp\PYZus{}b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Output of predict: shape }\PY{l+s+si}{\PYZob{}}\PY{n}{tmp\PYZus{}p}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, value }\PY{l+s+si}{\PYZob{}}\PY{n}{tmp\PYZus{}p}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} UNIT TESTS        }
\PY{n}{predict\PYZus{}test}\PY{p}{(}\PY{n}{predict}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Output of predict: shape (4,), value [0. 1. 1. 1.]
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected output}

Output of predict: shape (4,),value {[}0. 1. 1. 1.{]}

    Now let's use this to compute the accuracy on the training set

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Compute accuracy on our training set}
\PY{n}{p} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{w}\PY{p}{,}\PY{n}{b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{p} \PY{o}{==} \PY{n}{y\PYZus{}train}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Accuracy: 92.000000
    \end{Verbatim}

    Train Accuracy (approx):

92.00

    \#\# 3 - Regularized Logistic Regression

In this part of the exercise, you will implement regularized logistic
regression to predict whether microchips from a fabrication plant passes
quality assurance (QA). During QA, each microchip goes through various
tests to ensure it is functioning correctly.

\#\#\# 3.1 Problem Statement

Suppose you are the product manager of the factory and you have the test
results for some microchips on two different tests. - From these two
tests, you would like to determine whether the microchips should be
accepted or rejected. - To help you make the decision, you have a
dataset of test results on past microchips, from which you can build a
logistic regression model.

\#\#\# 3.2 Loading and visualizing the data

Similar to previous parts of this exercise, let's start by loading the
dataset for this task and visualizing it.

\begin{itemize}
\tightlist
\item
  The \texttt{load\_dataset()} function shown below loads the data into
  variables \texttt{X\_train} and \texttt{y\_train}

  \begin{itemize}
  \tightlist
  \item
    \texttt{X\_train} contains the test results for the microchips from
    two tests
  \item
    \texttt{y\_train} contains the results of the QA

    \begin{itemize}
    \tightlist
    \item
      \texttt{y\_train\ =\ 1} if the microchip was accepted
    \item
      \texttt{y\_train\ =\ 0} if the microchip was rejected
    \end{itemize}
  \item
    Both \texttt{X\_train} and \texttt{y\_train} are numpy arrays.
  \end{itemize}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} load dataset}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{load\PYZus{}data}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ex2data2.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{view-the-variables}{%
\paragraph{View the variables}\label{view-the-variables}}

The code below prints the first five values of \texttt{X\_train} and
\texttt{y\_train} and the type of the variables.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print X\PYZus{}train}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Type of X\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print y\PYZus{}train}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Type of y\PYZus{}train:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
X\_train: [[ 0.051267  0.69956 ]
 [-0.092742  0.68494 ]
 [-0.21371   0.69225 ]
 [-0.375     0.50219 ]
 [-0.51325   0.46564 ]]
Type of X\_train: <class 'numpy.ndarray'>
y\_train: [1. 1. 1. 1. 1.]
Type of y\_train: <class 'numpy.ndarray'>
    \end{Verbatim}

    \hypertarget{check-the-dimensions-of-your-variables}{%
\paragraph{Check the dimensions of your
variables}\label{check-the-dimensions-of-your-variables}}

Another useful way to get familiar with your data is to view its
dimensions. Let's print the shape of \texttt{X\_train} and
\texttt{y\_train} and see how many training examples we have in our
dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The shape of X\PYZus{}train is: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The shape of y\PYZus{}train is: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{We have m = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ training examples}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The shape of X\_train is: (118, 2)
The shape of y\_train is: (118,)
We have m = 118 training examples
    \end{Verbatim}

    \hypertarget{visualize-your-data}{%
\paragraph{Visualize your data}\label{visualize-your-data}}

The helper function \texttt{plot\_data} (from \texttt{utils.py}) is used
to generate a figure like Figure 3, where the axes are the two test
scores, and the positive (y = 1, accepted) and negative (y = 0,
rejected) examples are shown with different markers.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot examples}
\PY{n}{plot\PYZus{}data}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{pos\PYZus{}label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accepted}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{neg\PYZus{}label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rejected}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Set the y\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Microchip Test 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{c+c1}{\PYZsh{} Set the x\PYZhy{}axis label}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Microchip Test 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{upper right}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Figure 3 shows that our dataset cannot be separated into positive and
negative examples by a straight-line through the plot. Therefore, a
straight forward application of logistic regression will not perform
well on this dataset since logistic regression will only be able to find
a linear decision boundary.

    \#\#\# 3.3 Feature mapping

One way to fit the data better is to create more features from each data
point. In the provided function \texttt{map\_feature}, we will map the
features into all polynomial terms of \(x_1\) and \(x_2\) up to the
sixth power.

\[\mathrm{map\_feature}(x) = 
\left[\begin{array}{c}
x_1\\
x_2\\
x_1^2\\
x_1 x_2\\
x_2^2\\
x_1^3\\
\vdots\\
x_1 x_2^5\\
x_2^6\end{array}\right]\]

As a result of this mapping, our vector of two features (the scores on
two QA tests) has been transformed into a 27-dimensional vector.

\begin{itemize}
\tightlist
\item
  A logistic regression classifier trained on this higher-dimension
  feature vector will have a more complex decision boundary and will be
  nonlinear when drawn in our 2-dimensional plot.
\item
  We have provided the \texttt{map\_feature} function for you in
  utils.py.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original shape of data:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\PY{n}{mapped\PYZus{}X} \PY{o}{=}  \PY{n}{map\PYZus{}feature}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape after feature mapping:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mapped\PYZus{}X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Original shape of data: (118, 2)
Shape after feature mapping: (118, 27)
    \end{Verbatim}

    Let's also print the first elements of \texttt{X\_train} and
\texttt{mapped\_X} to see the tranformation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}train[0]:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mapped X\PYZus{}train[0]:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mapped\PYZus{}X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
X\_train[0]: [0.051267 0.69956 ]
mapped X\_train[0]: [5.12670000e-02 6.99560000e-01 2.62830529e-03 3.58643425e-02
 4.89384194e-01 1.34745327e-04 1.83865725e-03 2.50892595e-02
 3.42353606e-01 6.90798869e-06 9.42624411e-05 1.28625106e-03
 1.75514423e-02 2.39496889e-01 3.54151856e-07 4.83255257e-06
 6.59422333e-05 8.99809795e-04 1.22782870e-02 1.67542444e-01
 1.81563032e-08 2.47750473e-07 3.38066048e-06 4.61305487e-05
 6.29470940e-04 8.58939846e-03 1.17205992e-01]
    \end{Verbatim}

    While the feature mapping allows us to build a more expressive
classifier, it is also more susceptible to overfitting. In the next
parts of the exercise, you will implement regularized logistic
regression to fit the data and also see for yourself how regularization
can help combat the overfitting problem.

\#\#\# 3.4 Cost function for regularized logistic regression

In this part, you will implement the cost function for regularized
logistic regression.

Recall that for regularized logistic regression, the cost function is of
the form
\[J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]

Compare this to the cost function without regularization (which you
implemented above), which is of the form

\[ J(\mathbf{w}.b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]\]

The difference is the regularization term, which is
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\] Note that the \(b\)
parameter is not regularized.

    \#\#\# Exercise 5

Please complete the \texttt{compute\_cost\_reg} function below to
calculate the following term for each element in \(w\)
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]

The starter code then adds this to the cost without regularization
(which you computed above in \texttt{compute\_cost}) to calculate the
cost with regulatization.

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C5}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}cost\PYZus{}reg}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the cost over all examples}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      X : (array\PYZus{}like Shape (m,n)) data, m examples by n features}
\PY{l+s+sd}{      y : (array\PYZus{}like Shape (m,)) target value }
\PY{l+s+sd}{      w : (array\PYZus{}like Shape (n,)) Values of parameters of the model      }
\PY{l+s+sd}{      b : (array\PYZus{}like Shape (n,)) Values of bias parameter of the model}
\PY{l+s+sd}{      lambda\PYZus{} : (scalar, float)    Controls amount of regularization}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{      total\PYZus{}cost: (scalar)         cost }
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{n}{m}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}
    
    \PY{c+c1}{\PYZsh{} Calls the compute\PYZus{}cost function that you implemented above}
    \PY{n}{cost\PYZus{}without\PYZus{}reg} \PY{o}{=} \PY{n}{compute\PYZus{}cost}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)} 
    
    \PY{c+c1}{\PYZsh{} You need to calculate this value}
    \PY{n}{reg\PYZus{}cost} \PY{o}{=} \PY{l+m+mf}{0.}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{n}{reg\PYZus{}cost} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{} }
    
    \PY{c+c1}{\PYZsh{} Add the regularization cost to get the total cost}
    \PY{n}{total\PYZus{}cost} \PY{o}{=} \PY{n}{cost\PYZus{}without\PYZus{}reg} \PY{o}{+} \PY{p}{(}\PY{n}{lambda\PYZus{}}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{m}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{n}{reg\PYZus{}cost}

    \PY{k}{return} \PY{n}{total\PYZus{}cost}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  Here's how you can structure the overall implementation for this
  function ```python def compute\_cost\_reg(X, y, w, b, lambda\_ = 1):

\begin{verbatim}
     m, n = X.shape

      # Calls the compute_cost function that you implemented above
      cost_without_reg = compute_cost(X, y, w, b) 

      # You need to calculate this value
      reg_cost = 0.

      ### START CODE HERE ###
      for j in range(n):
          reg_cost_j = # Your code here to calculate the cost from w[j]
          reg_cost = reg_cost + reg_cost_j

      ### END CODE HERE ### 

      # Add the regularization cost to get the total cost
      total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost

  return total_cost
\end{verbatim}

  ```

  If you're still stuck, you can check the hints presented below to
  figure out how to calculate \texttt{reg\_cost\_j}

  Hint to calculate reg\_cost\_j     You can use calculate reg\_cost\_j
  as reg\_cost\_j = w{[}j{]}**2
\end{itemize}

    Run the cell below to check your implementation of the
\texttt{compute\_cost\_reg} function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}mapped} \PY{o}{=} \PY{n}{map\PYZus{}feature}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.5}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{cost} \PY{o}{=} \PY{n}{compute\PYZus{}cost\PYZus{}reg}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Regularized cost :}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cost}\PY{p}{)}

\PY{c+c1}{\PYZsh{} UNIT TEST    }
\PY{n}{compute\PYZus{}cost\PYZus{}reg\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}cost\PYZus{}reg}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Regularized cost : 0.6618252552483948
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected Output}:

Regularized cost :

0.6618252552483948

    \#\#\# 3.5 Gradient for regularized logistic regression

In this section, you will implement the gradient for regularized
logistic regression.

The gradient of the regularized cost function has two components. The
first, \(\frac{\partial J(\mathbf{w},b)}{\partial b}\) is a scalar, the
other is a vector with the same shape as the parameters \(\mathbf{w}\),
where the \(j^\mathrm{th}\) element is defined as follows:

\[\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})  \]

\[\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \left( \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \right) + \frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]

Compare this to the gradient of the cost function without regularization
(which you implemented above), which is of the form \[
\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}
\] \[
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}
\]

As you can see,\(\frac{\partial J(\mathbf{w},b)}{\partial b}\) is the
same, the difference is the following term in
\(\frac{\partial J(\mathbf{w},b)}{\partial w}\), which is
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]

    \#\#\# Exercise 6

Please complete the \texttt{compute\_gradient\_reg} function below to
modify the code below to calculate the following term

\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]

The starter code will add this term to the
\(\frac{\partial J(\mathbf{w},b)}{\partial w}\) returned from
\texttt{compute\_gradient} above to get the gradient for the regularized
cost function.

If you get stuck, you can check out the hints presented after the cell
below to help you with the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C6}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}gradient\PYZus{}reg}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:} 
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the gradient for linear regression }
\PY{l+s+sd}{ }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{      X : (ndarray Shape (m,n))   variable such as house size }
\PY{l+s+sd}{      y : (ndarray Shape (m,))    actual value }
\PY{l+s+sd}{      w : (ndarray Shape (n,))    values of parameters of the model      }
\PY{l+s+sd}{      b : (scalar)                value of parameter of the model  }
\PY{l+s+sd}{      lambda\PYZus{} : (scalar,float)    regularization constant}
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{      dj\PYZus{}db: (scalar)             The gradient of the cost w.r.t. the parameter b. }
\PY{l+s+sd}{      dj\PYZus{}dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. }

\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{m}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}
    
    \PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{compute\PYZus{}gradient}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}     }
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
        \PY{n}{dj\PYZus{}dw}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{dj\PYZus{}dw}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{lambda\PYZus{}}\PY{o}{/}\PY{n}{m}\PY{p}{)} \PY{o}{*} \PY{n}{w}\PY{p}{[}\PY{n}{j}\PY{p}{]}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}         }
        
    \PY{k}{return} \PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw}
\end{Verbatim}
\end{tcolorbox}

    Click for hints

\begin{itemize}
\item
  Here's how you can structure the overall implementation for this
  function ```python def compute\_gradient\_reg(X, y, w, b, lambda\_ =
  1): m, n = X.shape

\begin{verbatim}
  dj_db, dj_dw = compute_gradient(X, y, w, b)

  ### START CODE HERE ###     
  # Loop over the elements of w
  for j in range(n): 

      dj_dw_j_reg = # Your code here to calculate the regularization term for dj_dw[j]

      # Add the regularization term  to the correspoding element of dj_dw
      dj_dw[j] = dj_dw[j] + dj_dw_j_reg

  ### END CODE HERE ###         

  return dj_db, dj_dw
\end{verbatim}

  ```

  If you're still stuck, you can check the hints presented below to
  figure out how to calculate \texttt{dj\_dw\_j\_reg}

  Hint to calculate dj\_dw\_j\_reg     You can use calculate
  dj\_dw\_j\_reg as dj\_dw\_j\_reg = (lambda\_ / m) * w{[}j{]}
\end{itemize}

    Run the cell below to check your implementation of the
\texttt{compute\_gradient\_reg} function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}mapped} \PY{o}{=} \PY{n}{map\PYZus{}feature}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)} 
\PY{n}{initial\PYZus{}w}  \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.5} 
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{0.5}
 
\PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{dj\PYZus{}db}\PY{p}{,} \PY{n}{dj\PYZus{}dw} \PY{o}{=} \PY{n}{compute\PYZus{}gradient\PYZus{}reg}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dj\PYZus{}db: }\PY{l+s+si}{\PYZob{}}\PY{n}{dj\PYZus{}db}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First few elements of regularized dj\PYZus{}dw:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{dj\PYZus{}dw}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{p}{)}

\PY{c+c1}{\PYZsh{} UNIT TESTS    }
\PY{n}{compute\PYZus{}gradient\PYZus{}reg\PYZus{}test}\PY{p}{(}\PY{n}{compute\PYZus{}gradient\PYZus{}reg}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dj\_db: 0.07138288792343662
First few elements of regularized dj\_dw:
 [-0.010386028450548701, 0.011409852883280122, 0.0536273463274574,
0.0031402782673134655]
\textcolor{ansi-green-intense}{All tests passed!}
    \end{Verbatim}

    \textbf{Expected Output}:

dj\_db:0.07138288792343656

First few elements of regularized dj\_dw:

{[}{[}-0.010386028450548701{]}, {[}0.01140985288328012{]},
{[}0.0536273463274574{]}, {[}0.003140278267313462{]}{]}

    \#\#\# 3.6 Learning parameters using gradient descent

Similar to the previous parts, you will use your gradient descent
function implemented above to learn the optimal parameters \(w\),\(b\).
- If you have completed the cost and gradient for regularized logistic
regression correctly, you should be able to step through the next cell
to learn the parameters \(w\). - After training our parameters, we will
use it to plot the decision boundary.

\textbf{Note}

The code block below takes quite a while to run, especially with a
non-vectorized version. You can reduce the \texttt{iterations} to test
your implementation and iterate faster. If you hae time, run for 100,000
iterations to see better results.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Initialize fitting parameters}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{initial\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}
\PY{n}{initial\PYZus{}b} \PY{o}{=} \PY{l+m+mf}{1.}

\PY{c+c1}{\PYZsh{} Set regularization parameter lambda\PYZus{} to 1 (you can try varying this)}
\PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{l+m+mf}{0.01}\PY{p}{;}                                          
\PY{c+c1}{\PYZsh{} Some gradient descent settings}
\PY{n}{iterations} \PY{o}{=} \PY{l+m+mi}{10000}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n}{w}\PY{p}{,}\PY{n}{b}\PY{p}{,} \PY{n}{J\PYZus{}history}\PY{p}{,}\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{initial\PYZus{}w}\PY{p}{,} \PY{n}{initial\PYZus{}b}\PY{p}{,} 
                                    \PY{n}{compute\PYZus{}cost\PYZus{}reg}\PY{p}{,} \PY{n}{compute\PYZus{}gradient\PYZus{}reg}\PY{p}{,} 
                                    \PY{n}{alpha}\PY{p}{,} \PY{n}{iterations}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
    \end{Verbatim}

    Expected Output: Cost \textless{} 0.5 (Click for details)

\begin{verbatim}
# Using the following settings
#np.random.seed(1)
#initial_w = np.random.rand(X_mapped.shape[1])-0.5
#initial_b = 1.
#lambda_ = 0.01;                                          
#iterations = 10000
#alpha = 0.01
Iteration    0: Cost     0.72   
Iteration 1000: Cost     0.59   
Iteration 2000: Cost     0.56   
Iteration 3000: Cost     0.53   
Iteration 4000: Cost     0.51   
Iteration 5000: Cost     0.50   
Iteration 6000: Cost     0.48   
Iteration 7000: Cost     0.47   
Iteration 8000: Cost     0.46   
Iteration 9000: Cost     0.45   
Iteration 9999: Cost     0.45       
    
\end{verbatim}

    \#\#\# 3.7 Plotting the decision boundary To help you visualize the
model learned by this classifier, we will use our
\texttt{plot\_decision\_boundary} function which plots the (non-linear)
decision boundary that separates the positive and negative examples.

\begin{itemize}
\item
  In the function, we plotted the non-linear decision boundary by
  computing the classifier's predictions on an evenly spaced grid and
  then drew a contour plot of where the predictions change from y = 0 to
  y = 1.
\item
  After learning the parameters \(w\),\(b\), the next step is to plot a
  decision boundary similar to Figure 4.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plot\PYZus{}decision\PYZus{}boundary}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{X\PYZus{}mapped}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \#\#\# 3.8 Evaluating regularized logistic regression model

You will use the \texttt{predict} function that you implemented above to
calculate the accuracy of the regulaized logistic regression model on
the training set

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Compute accuracy on the training set}
\PY{n}{p} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}mapped}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{p} \PY{o}{==} \PY{n}{y\PYZus{}train}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train Accuracy: 82.203390
    \end{Verbatim}

    \textbf{Expected Output}:

Train Accuracy:\textasciitilde{} 80\%

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
